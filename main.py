# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1es5OX9n3kjXLB38hd1uTendCapcqMisI
"""

from gpt2small import *
from gpt2medium import *
from llama import *
from evaluation import *
import torch
import numpy as np
from transformers import AutoModelForCausalLM, AutoTokenizer
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
from IPython.display import HTML, display
import re
from transformers import GPT2LMHeadModel, GPT2Tokenizer
import seaborn as sns
import matplotlib.colors as mcolors



if __name__ == "__main__":
    model_name = "gpt2-small"  # Example model
    sentences = [
        "The movie was fantastic and very well-directed.",
        "I did not enjoy the film. It was boring and predictable.",
        "An absolute masterpiece. Brilliant storytelling and visuals.",
    ]
    sentence = sentences[0]
    word_index = 6
    if model_name == "gpt2-small":
        model = GPT2smallAIEG(sentence,word_index)
    elif model_name == "gpt2-medium":
        model = GPT2mediumAIEG(sentence,word_index)
    else:
        model = LlamaAIEG(sentence,word_index)


    model_name = 'gpt2'
    tokenizer = GPT2Tokenizer.from_pretrained(model_name)
    model = GPT2LMHeadModel.from_pretrained(model_name, output_attentions=True)

    k = 20  # Top 20% of words
    target_class = None  # Use the predicted class

    log_odds, comp, suff = compute_scores(model, tokenizer, sentences, k, target_class)
    print(f"Log-Odds Score (k={k}%): {log_odds}")
    print(f"Comprehensiveness Score (k={k}%): {comp}")
    print(f"Sufficiency Score (k={k}%): {suff}")