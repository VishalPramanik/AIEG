{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import matplotlib.colors as mcolors\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "\n",
        "def AIEG(generated_text, word_index):\n",
        "  # Step 1: Load the pre-trained GPT-2 model and tokenizer\n",
        "  model_name = 'gpt2'  # You can also use 'gpt2-medium', 'gpt2-large', etc.\n",
        "  tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "  model = GPT2LMHeadModel.from_pretrained(model_name, output_attentions=True)\n",
        "\n",
        "  # Set the model to evaluation mode (not training)\n",
        "  model.eval()\n",
        "\n",
        "  def integrated_gradients(inputs, model, tokenizer, t_idx, baseline=None, steps=50):\n",
        "      input_ids = tokenizer.encode(inputs, return_tensors=\"pt\")\n",
        "\n",
        "      # Get the embeddings from the model's embedding layer\n",
        "      embedding_layer = model.transformer.wte\n",
        "\n",
        "      if baseline is None:\n",
        "          baseline = torch.zeros_like(embedding_layer(input_ids))\n",
        "\n",
        "      total_gradients = torch.zeros_like(embedding_layer(input_ids))\n",
        "\n",
        "      for alpha in torch.linspace(0, 1, steps):\n",
        "          interpolated_input = baseline + alpha * (embedding_layer(input_ids) - baseline)\n",
        "          interpolated_input.retain_grad()  # Ensure gradients are retained for non-leaf tensor\n",
        "\n",
        "          # Perform a forward pass with the interpolated input\n",
        "          outputs = model(inputs_embeds=interpolated_input)[0]\n",
        "          token_idx = t_idx  # Analyze the token\n",
        "          output_score = outputs[0, token_idx].sum()\n",
        "\n",
        "          if alpha!=0:\n",
        "            EF = (output_score - prev)/(output_score + prev) # the EF factor\n",
        "          prev=output_score\n",
        "\n",
        "\n",
        "          # Compute gradients with respect to the interpolated input\n",
        "          output_score.backward()\n",
        "          gradients = interpolated_input.grad\n",
        "\n",
        "          if alpha!=0:\n",
        "            gradients = gradients*abs(EF.item())\n",
        "\n",
        "          # Accumulate the gradients\n",
        "\n",
        "          total_gradients += (gradients)\n",
        "\n",
        "      final_gradients = total_gradients\n",
        "\n",
        "      # Multiply the average gradients by the difference between input embeddings and baseline\n",
        "      output_X_EF = (embedding_layer(input_ids) - baseline) * final_gradients\n",
        "      return output_X_EF\n",
        "\n",
        "\n",
        "  # Define your input text\n",
        "  input_text = generated_text\n",
        "\n",
        "  # Calculate modified integrated gradients\n",
        "  ig = integrated_gradients(input_text, model, tokenizer, word_index)\n",
        "\n",
        "  # Convert gradients to a more interpretable form\n",
        "  ig_scores = ig.squeeze().sum(dim=-1).detach().numpy()  # Sum across embedding dimensions\n",
        "  tokens = tokenizer.convert_ids_to_tokens(tokenizer.encode(input_text))\n",
        "  ig_scores[word_index] = 0\n",
        "\n",
        "\n",
        "  # Display the tokens and their corresponding IG scores\n",
        "  print(\"Output x EF Scores\")\n",
        "  for token, score in zip(tokens, ig_scores):\n",
        "      print(f\"{token}: {score}\")\n",
        "\n",
        "\n",
        "  # removing all the negative attributions\n",
        "  ig_scores = np.where(ig_scores < 0, 0, ig_scores)\n",
        "\n",
        "  # Normalisation Values\n",
        "  total_sum = np.sum(ig_scores)\n",
        "  normalized_scores = ig_scores / total_sum\n",
        "\n",
        "  # Display the tokens and their corresponding Normalised scores\n",
        "  print(\"Normalised Scores\")\n",
        "  for token, score in zip(tokens, normalized_scores):\n",
        "      print(f\"{token}: {score}\")\n",
        "\n",
        "\n",
        "  # Tokenize input text\n",
        "  input_text = generated_text\n",
        "  input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "\n",
        "\n",
        "\n",
        "  ###############################################################\n",
        "  # Calculating the attention values from all the layes and heads\n",
        "  # Pass input through the model\n",
        "  outputs = model(input_ids)\n",
        "  attentions = outputs.attentions\n",
        "  # Choose the layer and head to inspect\n",
        "  sum_of_attentions=[]\n",
        "  number_of_layers = 12\n",
        "  number_of_heads = 12\n",
        "  for layer in range(number_of_layers):\n",
        "    temp = []\n",
        "    for head in range(number_of_heads):\n",
        "      # Get the attention matrix for the chosen layer and head\n",
        "      attention_matrix = attentions[layer][0, head]\n",
        "\n",
        "      # Get the attention values for the chosen word with respect to all other words\n",
        "      attention_values = attention_matrix[word_index].detach().numpy()\n",
        "\n",
        "      temp.append(attention_values)\n",
        "\n",
        "    sum_of_attentions.append((np.sum(temp, axis=0))/number_of_heads)\n",
        "\n",
        "  # Convert token IDs back to words\n",
        "  tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
        "  # prompt: sum of all the array in the temp list\n",
        "\n",
        "  sum_of_attentions = np.sum(temp, axis=0)\n",
        "  sum_of_attentions/=number_of_layers\n",
        "  # Display the tokens and their corresponding Attention values\n",
        "  temp = sum_of_attentions\n",
        "  print(\"Attention\")\n",
        "  for token, score in zip(tokens, temp):\n",
        "      print(f\"{token}: {score}\")\n",
        "\n",
        "  temp = sum_of_attentions\n",
        "\n",
        "  # Calculating the corresponding AIEG scores for the tokens\n",
        "  contribution_scores = []\n",
        "  for i in range(len(sum_of_attentions)):\n",
        "      contribution_scores.append(sum_of_attentions[i] * normalized_scores[i])\n",
        "\n",
        "\n",
        "  token_contribution_dict = dict(zip(tokens, contribution_scores))\n",
        "  # Displaying the AIEG Contribution Score for the tokens\n",
        "  temp = contribution_scores\n",
        "  print(\"Contribution Scores\")\n",
        "  for token, score in zip(tokens, temp):\n",
        "      print(f\"{token}: {score}\")\n",
        "\n",
        "\n",
        "\n",
        "  ##################################################################################################################################\n",
        "  # This portion of the code merge the tokens of a word(if splitted during tokenisation) and also converts the tokens into words.\n",
        "  # It also adds the contribution values of the splitted words into a single value.\n",
        "  # Initialize variables\n",
        "  word_contributions = {}\n",
        "  current_word = \"\"\n",
        "  current_score = 0.0\n",
        "\n",
        "  for token, score in token_contribution_dict.items():\n",
        "      # Check if the token starts with Ġ (space) or is a new word\n",
        "      if token.startswith('Ġ') or token.startswith('Ċ') or (current_word != \"\" and not re.match(r'\\w', token)):\n",
        "          # If there's an existing word being built, store it\n",
        "          if current_word:\n",
        "              word_contributions[current_word] = current_score\n",
        "\n",
        "          # Start a new word\n",
        "          current_word = token.lstrip('ĠĊ')\n",
        "          current_score = score\n",
        "      else:\n",
        "          # Continue building the current word\n",
        "          current_word += token\n",
        "          current_score += score\n",
        "\n",
        "  # Add the last word to the dictionary\n",
        "  if current_word:\n",
        "      word_contributions[current_word] = current_score\n",
        "\n",
        "  # Output the final dictionary of words and their summed contribution scores\n",
        "  print(word_contributions)\n",
        "  print(generated_text)\n",
        "\n",
        "\n",
        "  ##########################################################################\n",
        "  # In this section we do the color coding of the text with their AIEG values\n",
        "  def plot_word_contributions(zip_object):\n",
        "      # Convert zip object to dictionary\n",
        "      contributions = dict(zip_object)\n",
        "\n",
        "      # Sort contributions by score (if desired)\n",
        "      sorted_contributions = contributions  # or dict(sorted(contributions.items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "      # Create a color map that ranges from light green to dark green\n",
        "      cmap = plt.get_cmap(\"Greens\")\n",
        "\n",
        "      # Get the contribution scores and normalize them\n",
        "      scores = list(sorted_contributions.values())\n",
        "      max_score = max(scores)\n",
        "      min_score = min(scores)\n",
        "\n",
        "      def score_to_color(score):\n",
        "          # Normalize score between 0 and 1\n",
        "          norm_score = (score - min_score) / (max_score - min_score)\n",
        "          # Convert normalized score to a color (light green to dark green)\n",
        "          color = cmap(norm_score)\n",
        "          return mcolors.to_hex(color[:3])  # Convert RGB to HEX\n",
        "\n",
        "      # Generate HTML-like string for visualization\n",
        "      html_output = \"<html><body>\"\n",
        "      for word, score in sorted_contributions.items():\n",
        "          color_hex = score_to_color(score)\n",
        "          html_output += f'<span style=\"background-color: {color_hex}; color: black; font-size: 20px; margin-right: 5px; padding: 2px; border-radius: 3px;\">{word}</span>'\n",
        "      html_output += \"</body></html>\"\n",
        "\n",
        "      # Display HTML output in the notebook\n",
        "      display(HTML(html_output))\n",
        "\n",
        "  # Call the function to create the visualization\n",
        "  plot_word_contributions(word_contributions)"
      ],
      "metadata": {
        "id": "fITm7v3jpQ_S"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputsentence = \"This is a vey good movie\"\n",
        "word_of_interest = 4\n",
        "AIEG(inputsentence, word_of_interest)"
      ],
      "metadata": {
        "id": "RF3NGoYDxdTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3mTf3qeR6zHR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}