# -*- coding: utf-8 -*-
"""Evaluation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dlIERM-to4mEzIm3HXRLBg_3yjPnNTTq
"""

import numpy as np
import torch
from transformers import AutoModelForSequenceClassification, AutoTokenizer
from scipy.special import softmax

def compute_scores(model, tokenizer, sentences, k, target_class=None):
    """
    Compute Log-Odds (LO), Comprehensiveness (Comp), and Sufficiency (Suff) scores for a given dataset.

    Parameters:
        model (transformers.AutoModelForSequenceClassification): Pretrained classification model.
        tokenizer (transformers.AutoTokenizer): Tokenizer corresponding to the model.
        sentences (list): List of sentences to evaluate.
        k (float): Percentage of top features to mask, delete, or keep (0 < k <= 100).
        target_class (int, optional): The target class for which probabilities are computed.
                                      If None, uses the predicted class for each sentence.

    Returns:
        tuple: Average Log-Odds, Comprehensiveness, and Sufficiency scores.
    """
    model.eval()
    total_log_odds = 0
    total_comp = 0
    total_suff = 0
    N = len(sentences)

    for sentence in sentences:
        # Tokenize and encode the original sentence
        inputs = tokenizer(sentence, return_tensors="pt", truncation=True, padding=True)
        input_ids = inputs["input_ids"]

        # Get model outputs and probabilities for the original sentence
        with torch.no_grad():
            original_outputs = model(**inputs).logits
        original_probs = softmax(original_outputs.numpy(), axis=1)
        original_class = original_probs.argmax() if target_class is None else target_class

        # Compute attributions (dummy attributions for simplicity, replace with a real method)
        attributions = np.random.rand(input_ids.size(1))  # Dummy attributions
        top_k_indices = np.argsort(attributions)[-int(k * len(attributions) / 100):]

        # Mask top k% tokens (Log-Odds)
        masked_input_ids = input_ids.clone()
        for idx in top_k_indices:
            masked_input_ids[0, idx] = tokenizer.pad_token_id  # Replace with padding token

        with torch.no_grad():
            masked_inputs = {"input_ids": masked_input_ids, "attention_mask": inputs["attention_mask"]}
            masked_outputs = model(**masked_inputs).logits
        masked_probs = softmax(masked_outputs.numpy(), axis=1)

        # Compute log-odds
        p_original = original_probs[0, original_class]
        p_masked = masked_probs[0, original_class]
        log_odds = np.log(p_masked / p_original)
        total_log_odds += log_odds

        # Remove top k% tokens (Comprehensiveness)
        deleted_input_ids = torch.cat([input_ids[0, :idx] for idx in range(len(input_ids[0])) if idx not in top_k_indices]).unsqueeze(0)

        with torch.no_grad():
            deleted_inputs = {"input_ids": deleted_input_ids, "attention_mask": inputs["attention_mask"]}
            deleted_outputs = model(**deleted_inputs).logits
        deleted_probs = softmax(deleted_outputs.numpy(), axis=1)

        # Compute comprehensiveness
        p_deleted = deleted_probs[0, original_class]
        comp_score = p_deleted - p_original
        total_comp += comp_score

        # Keep only top k% tokens (Sufficiency)
        kept_input_ids = torch.cat([input_ids[0, idx].unsqueeze(0) for idx in top_k_indices]).unsqueeze(0)

        with torch.no_grad():
            kept_inputs = {"input_ids": kept_input_ids, "attention_mask": torch.ones_like(kept_input_ids)}
            kept_outputs = model(**kept_inputs).logits
        kept_probs = softmax(kept_outputs.numpy(), axis=1)

        # Compute sufficiency
        p_kept = kept_probs[0, original_class]
        suff_score = p_kept - p_original
        total_suff += suff_score

    # Return average scores
    avg_log_odds = total_log_odds / N
    avg_comp = total_comp / N
    avg_suff = total_suff / N
    return avg_log_odds, avg_comp, avg_suff


# Example usage
if __name__ == "__main__":
    model_name = "textattack/bert-base-uncased-SST-2"  # Example model (Sentiment Analysis)
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSequenceClassification.from_pretrained(model_name)

    sentences = [
        "The movie was fantastic and very well-directed.",
        "I did not enjoy the film. It was boring and predictable.",
        "An absolute masterpiece. Brilliant storytelling and visuals.",
    ]
    k = 10  # Top 10% of words
    target_class = None  # Use the predicted class

    log_odds, comp, suff = compute_scores(model, tokenizer, sentences, k, target_class)
    print(f"Log-Odds Score (k={k}%): {log_odds}")
    print(f"Comprehensiveness Score (k={k}%): {comp}")
    print(f"Sufficiency Score (k={k}%): {suff}")